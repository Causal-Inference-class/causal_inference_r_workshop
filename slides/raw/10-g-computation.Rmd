---
title: "G-Computation"
author: "Malcolm Barrett"
institute: "RStudio, PBC"
date: "2021-09-01 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: ["default", "theme.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
      navigation:
        scroll: false 
---
class: middle, center, inverse, 

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, tibble.max_extra_cols = 6, tibble.width = 60)
knitr::opts_chunk$set(
  warning = FALSE, 
  message = FALSE, 
  dev = "ragg_png",
  dpi = 320,
  out.width = "80%",
  fig.width = 5,
  fig.asp = 0.618,
  fig.retina = 2,
  fig.align = "center",
  fig.show = "hold"
)
library(tidyverse)
library(broom)
library(causaldata)

set.seed(1000)
```

# Normal regression estimates associations. But we want *causal* estimates: what would happen if *everyone* in the study were exposed to x vs if *no one* was exposed.

---

class: inverse
# G-Computation/G-Formula
1. Fit a model for `y ~ x + z` where z is all covariates
2. Create a duplicate of your data set for each level of `x` 
3. Set the value of x to a single value for each cloned data set (e.g `x = 1` for one, `x = 0` for the other)

---

class: inverse
# G-Computation/G-Formula
4. Make predictions using the model on the cloned data sets
5. Calculate the estimate you want, e.g. `mean(x_1) - mean(x_0)`

---

## *Advantages of the parametric G-formula*

## Often more statistically precise than propensity-based methods
## Incredibly flexible
## Basis of other important causal models, e.g. causal survival analysis and TMLE

---

## Greek Pantheon data (`greek_data`)

```{r, echo=FALSE}
library(gt)
gt(head(greek_data, 10))
```

.pull-right[
\+ 10 more rows
]


---

## 1. Fit a model for `y ~ a + l`

```{r}
greek_model <- lm(y ~ a + l, data = greek_data)
```

---

## 2. Create a duplicate of your data set for each level of `a` 

```{r, echo=FALSE}
gt(head(greek_data, 10))
```

---

## 2. Create a duplicate of your data set for each level of `a` 

.pull-left[
```{r, echo=FALSE}
gt(head(greek_data, 10))
```
]

.pull-right[
```{r, echo=FALSE}
gt(head(greek_data, 10))
```
]
---

## 3. Set the value of `a` to a single value for each cloned data set

.pull-left[
```{r, echo=FALSE}
gt(head(greek_data %>% mutate(a = 0), 10)) %>% 
  tab_style(
    style = list(
      cell_fill(color = "#CEE9FF"),
      cell_text(weight = "bold")
      ),
    locations = cells_body(columns = a)
  )
```
]

.pull-right[
```{r, echo=FALSE}
gt(head(greek_data %>% mutate(a = 1), 10)) %>% 
  tab_style(
    style = list(
      cell_fill(color = "#CEE9FF"),
      cell_text(weight = "bold")
      ),
    locations = cells_body(columns = a)
  )
```
]
---

## 3. Set the value of `a` to a single value for each cloned data set

```{r}
#  set all participants to have a = 0
untreated_data <- greek_data %>% 
  mutate(a = 0) #<<

#  set all participants to have a = 1
treated_data <- greek_data %>% 
  mutate(a = 1) #<<
```

---

## 4. Make predictions using the model on the cloned data sets

```{r}
#  predict under the data where everyone is untreated
predicted_untreated <- greek_model %>% #<<
  augment(newdata = untreated_data) %>% #<<
  select(untreated = .fitted)

#  predict under the data where everyone is treated
predicted_treated <- greek_model %>% #<<
  augment(newdata = treated_data) %>% #<<
  select(treated = .fitted)

predictions <- bind_cols(
  predicted_untreated, 
  predicted_treated
) 
```

---

## 5. Calculate the estimate you want

```{r, eval = FALSE}
predictions %>% 
  summarise(
    mean_treated = mean(treated),
    mean_untreated = mean(untreated),
    difference = mean_treated - mean_untreated #<<
  )
```

```{r, echo = FALSE}
predictions %>% 
  summarise(
    mean_treated = mean(treated),
    mean_untreated = mean(untreated),
    difference = mean_treated - mean_untreated
  ) %>% 
  mutate(across(everything(), round, digits = 1))
```
---

## Your Turn

### Work through Your Turns 1-3 in `07-g-computation-exercises.Rmd`

`r countdown::countdown(minutes = 10)`

---

class: middle, center, inverse, 

# **Detour**: Colliders, selection bias, and loss to follow-up

---

## Confounders and chains

```{r, echo = FALSE}
library(ggdag)
ggdag_confounder_triangle(x_y_associated = TRUE) + 
  theme_dag() +
  expand_plot(expansion(.2), expansion(.2))
```

---

## Colliders

```{r, echo = FALSE}
ggdag_collider_triangle() + 
  theme_dag() +
  expand_plot(expansion(.2), expansion(.2))
```

---

## Colliders

```{r, echo = FALSE}
ggdag_adjust(
  collider_triangle(), 
  "m", 
  collider_lines = FALSE
) + 
  theme_dag() +
  theme(legend.position = "none") +
  expand_plot(expansion(.2), expansion(.2))
```

---

## Loss to follow-up

```{r, echo=FALSE}
l2fu <- dagify(follow_up ~ symptoms,
       symptoms ~ new_rx + dx_severity,
       cd4 ~ dx_severity,
       labels = c(
         follow_up = "Follow-Up",
         symptoms = "Symptoms",
         new_rx = "New HIV Drug",
         dx_severity = "Underyling \nHIV Severity",
         cd4 = "CD4 Count"
       ), exposure = "new_rx", outcome = "cd4")

l2fu %>% 
  ggdag_adjust("follow_up", layout = "mds", text = FALSE, collider_lines = FALSE) + geom_dag_text_repel(aes(label = label), color = "black", point.padding = 100) + 
  theme_dag() +
  theme(legend.position = "none") +
  expand_plot()
```

---

class: inverse

# Adjusting for selection bias

1. Fit a probability of censoring model, e.g. *glm(censoring ~ predictors, family = binomial())*
2. Create weights using inverse probability strategy
3. Use weights in your causal model

---

class: middle, center, inverse

# We won't do it here, but you can include many types of weights in a given model. Just take their product, e.g. *multiply inverse propensity of treatment weights by inverse propensity of censoring weights*.

---

## Your Turn

### Work through Your Turns 4-6 in `07-g-computation-exercises.Rmd`

`r countdown::countdown(minutes = 10)`
